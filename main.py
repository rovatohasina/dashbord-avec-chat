import streamlit as st
import os
import json
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from datetime import datetime
from embedding.query_embeddings import query_pinecone
from data.wbdata_loader import get_live_wbdata

# Charger les variables d'environnement
load_dotenv()

st.set_page_config(page_title="Pr√©vision du PIB", layout="wide", initial_sidebar_state="expanded")

def create_prompt(question, wbdata_context, pdf_context):
    """
    Cr√©e un prompt structur√© pour r√©pondre √† une question sur un indicateur √©conomique sp√©cifique
    en n'utilisant que les donn√©es disponibles pour cet indicateur et cette ann√©e avec les donn√©es √©conomiques (wbdata) et les explications (pdf),
    sans utiliser de num√©rotation dans la r√©ponse.
    """
    wbdata_part = wbdata_context.strip()
    pdf_part = pdf_context.strip()

    if not wbdata_part and not pdf_part:
        return f"""
Tu es un assistant √©conomique. Voici la question de l'utilisateur :

{question}

R√©ponds de mani√®re fluide, claire et structur√©e. Si aucune information n‚Äôest disponible, r√©ponds simplement :
"Je ne dispose pas de cette information."
"""
    else:
        prompt = f"""
Tu es un assistant √©conomique intelligent. R√©ponds √† la question suivante avec clart√© et structure.

**Question :** {question}

**Instructions :**
- Commence par une salutation naturelle.
- R√©ponds uniquement √† **l'indicateur √©conomique demand√©** dans la question.
- Ne mentionne **aucun autre indicateur**, m√™me si des donn√©es sont disponibles.
- Ne parle que de l‚Äôann√©e explicitement demand√©e. Ignore les autres ann√©es.
- Si l‚Äôinformation est absente, √©cris : "Je ne dispose pas de cette information."
{wbdata_part}
- Ensuite, compl√®te avec des explications tir√©es uniquement de ce contexte :
{pdf_part}

- Ne devine rien : si une information n'est pas pr√©sente, indique-le simplement ("Donn√©e non disponible").
- Ne r√©p√®te pas la question. Concentre-toi sur une r√©ponse directe, fluide et professionnelle.
"""
        return prompt


def ask_gemini(prompt):
    """
    Envoie le prompt √† Gemini et retourne la r√©ponse g√©n√©r√©e avec une bonne structure de phrase.
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    response = llm.invoke([HumanMessage(content=prompt)])
    return response

def save_conversation(question, response, file_path="logs/conversation_log.json"):
    if not os.path.exists("logs"):
        os.makedirs("logs")

    response_text = response.content if hasattr(response, "content") else str(response)

    conversation = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "response": response_text
    }

    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError:
            data = []
    else:
        data = []

    data.append(conversation)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def main():
    st.markdown("""
        <style>
            .chat-bubble-user {
                background-color: black;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 5px;
                text-align: right;
            }
            .chat-bubble-bot {
                background-color: black;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 5px;
                text-align: left;
            }
            .title {
                font-size: 32px;
                font-weight: bold;
                color: #004080;
            }
        </style>
    """, unsafe_allow_html=True)
# Initialisation de l'historique
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Formulaire de saisie dans la sidebar
df_pivot,filtered_data,forecast_df_PIB, filtered_df_depenses, filtered_df_recettes, filtered_ventillation,df_sunburst,selected_Ann√©es,selected_Ann√©e_ventillation,selected_Ann√©e,selected_Ann√©e_depenses,selected_Ann√©e_recettes=get_live_wbdata()
with st.sidebar.form("chat_form"):
    user_question = st.text_input("Votre question :", key="user_input")
    submitted = st.form_submit_button("Envoyer")

if submitted and (user_question or selected_Ann√©es or selected_Ann√©e_ventillation or selected_Ann√©e or selected_Ann√©e_depenses or selected_Ann√©e_recettes):
    with st.spinner("Analyse en cours..."):
        wbdata_context, pdf_context = query_pinecone(user_question,selected_Ann√©es,selected_Ann√©e_ventillation,selected_Ann√©e,selected_Ann√©e_depenses,selected_Ann√©e_recettes)
        prompt = create_prompt(user_question, wbdata_context, pdf_context)
        try:
            response = ask_gemini(prompt)
            response_text = response.content
        except Exception as e:
            response_text = f"Erreur : {e}"
    # Ajouter √† l'historique
    st.session_state.chat_history.append(("Vous", user_question))
    st.session_state.chat_history.append(("Chat_economique", response_text))
    # Sauvegarde
    save_conversation(user_question, response)
    st.session_state.submit = False
if "chat_history" in st.session_state and st.session_state.chat_history:
    for sender, msg in st.session_state.chat_history:
        bubble_class = "chat-bubble-user" if sender == "Vous" else "chat-bubble-bot"
        st.sidebar.markdown(f'<div class="{bubble_class}"><b>{sender} :</b><br>{msg}</div>', unsafe_allow_html=True)
else:
    st.sidebar.info("Aucune conversation pour le moment")
if __name__ == "__main__":
    main()

# def main():
#     st.markdown("""
#         <style>
#         #chat-toggle-btn {
#             position: fixed;
#             bottom: 20px;   /* distance du bas */
#             right: 20px;    /* distance de la droite */
#             z-index: 9999;  /* pour qu‚Äôil soit au-dessus */
#             background-color: #007bff;
#             color: white;
#             border: none;
#             padding: 12px 16px;
#             border-radius: 50%;
#             font-size: 24px;
#             cursor: pointer;
#             box-shadow: 0 4px 6px rgba(0,0,0,0.1);
#         }
#             #chat-toggle-btn:hover {
#             background-color: #0056b3;
#         }
#         </style>
#         <button id="chat-toggle-btn">üí¨</button>
#         <script>
#         const btn = window.parent.document.getElementById('chat-toggle-btn');
#         const chatWindow = window.parent.document.getElementById('chat-window');

#         btn.onclick = () => {
#             if (chatWindow.style.display === 'none' || chatWindow.style.display === '') {
#                 chatWindow.style.display = 'block';
#             } else {
#                 chatWindow.style.display = 'none';
#             }
#         }
#         </script>
#     """, unsafe_allow_html=True)

# df_pivot,filtered_data,forecast_df_PIB, filtered_df_depenses, filtered_df_recettes, filtered_ventillation,df_sunburst,selected_Ann√©es,selected_Ann√©e_ventillation,selected_Ann√©e,selected_Ann√©e_depenses,selected_Ann√©e_recettes = get_live_wbdata()
# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []
# if "chat_visible" not in st.session_state:
#     st.session_state.chat_visible = False
# toggle_btn = st.button("üí¨", key="toggle_chat_btn")
# if toggle_btn:
#     st.session_state.chat_visible = not st.session_state.chat_visible
# chat_container = st.empty()
# if st.session_state.chat_visible:
#     with chat_container.container():
#         with st.form("chat_form", clear_on_submit=True):
#             user_question = st.text_input("Votre question :", key="user_input")
#             submitted = st.form_submit_button("Envoyer")
#             if submitted and user_question.strip():
#                 with st.spinner("Analyse en cours..."):
#                     wbdata_context, pdf_context = query_pinecone(user_question, selected_Ann√©es, selected_Ann√©e_ventillation, selected_Ann√©e, selected_Ann√©e_depenses, selected_Ann√©e_recettes)
#                     prompt = create_prompt(user_question, wbdata_context, pdf_context)
#                     try:
#                         response = ask_gemini(prompt)
#                         response_text = response.content
#                     except Exception as e:
#                         response_text = f"Erreur : {e}"
#                 st.session_state.chat_history.append(("Vous", user_question))
#                 st.session_state.chat_history.append(("Chat_economique", response_text))
#                 save_conversation(user_question, response)
#     for sender, msg in st.session_state.chat_history:
#         bubble_class = "chat-bubble-user" if sender == "Vous" else "chat-bubble-bot"
#         st.markdown(f'<div class="{bubble_class}"><b>{sender} :</b><br>{msg}</div>', unsafe_allow_html=True)
# if __name__ == "__main__":
#     main()

